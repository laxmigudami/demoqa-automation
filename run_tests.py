import argparse
import subprocess
import sys
from datetime import datetime
from pathlib import Path


class TestRunner:
    """Test runner class to manage test execution"""

    def __init__(self):
        """Initialize test runner"""
        self.project_root = Path(__file__).parent
        self.reports_dir = self.project_root / "reports"
        self.logs_dir = self.project_root / "logs"
        self.screenshots_dir = self.reports_dir / "screenshots"
        self.allure_results_dir = self.reports_dir / "allure_reports"
        self.start_time = None
        self.end_time = None

    def setup_directories(self):
        """Create necessary directories for reports and logs"""
        print("Setting up directories...")

        directories = [self.reports_dir, self.logs_dir, self.screenshots_dir, self.allure_results_dir]

        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            print(f"   Created: {directory}")

        print()

    def print_header(self):
        """Print test execution header"""
        print("\n" + "=" * 80)
        print("TEST EXECUTION STARTED")
        print("=" * 80)
        print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Project Root: {self.project_root}")
        print("=" * 80 + "\n")

    def print_footer(self, return_code):
        """Print test execution footer"""
        self.end_time = datetime.now()
        execution_time = (self.end_time - self.start_time).total_seconds()

        print("\n" + "=" * 80)

        if return_code == 0:
            print("TEST EXECUTION PASSED")
        else:
            print("TEST EXECUTION FAILED")

        print("=" * 80)
        print(f"Execution Time: {execution_time:.2f} seconds")
        print(f"End Time: {self.end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80 + "\n")

    def print_report_locations(self):
        """Print report locations and verify files exist"""
        print("=" * 80)
        print("GENERATED ARTIFACTS & REPORTS")
        print("=" * 80 + "\n")

        # Check and display Allure results
        print("Allure Results:")
        print(f"   Location: {self.allure_results_dir}")
        if self.allure_results_dir.exists():
            # Check for Allure result files (JSON files generated by allure-behave)
            allure_files = list(self.allure_results_dir.glob("*-result.json"))
            container_files = list(self.allure_results_dir.glob("*-container.json"))

            if allure_files or container_files:
                total_size = sum(f.stat().st_size for f in allure_files + container_files) / 1024
                print(f"   {len(allure_files)} test result(s) ({total_size:.2f} KB)")
                print(f"   {len(container_files)} container(s)")
            else:
                print("   WARNING: No allure results found")
        else:
            print("   WARNING: Allure results directory not found")
        print()

        # Check and display Screenshots
        print("Screenshots:")
        print(f"   Location: {self.screenshots_dir}")
        if self.screenshots_dir.exists():
            screenshots = list(self.screenshots_dir.glob("*.png"))
            if screenshots:
                print(f"   {len(screenshots)} screenshot(s) captured")
                for screenshot in screenshots[:5]:  # Show first 5
                    print(f"      - {screenshot.name}")
                if len(screenshots) > 5:
                    print(f"      ... and {len(screenshots) - 5} more")
            else:
                print("   No screenshots (tests passed or capture disabled)")
        print()

        # Check and display Logs
        print("Execution Logs:")
        print(f"   Location: {self.logs_dir}")
        if self.logs_dir.exists():
            log_files = list(self.logs_dir.glob("*.log"))
            if log_files:
                print(f"   {len(log_files)} log file(s) generated")
                # Show the most recent execution log
                exec_logs = sorted(
                    self.logs_dir.glob("test_execution_*.log"), key=lambda x: x.stat().st_mtime, reverse=True
                )
                if exec_logs:
                    latest_log = exec_logs[0]
                    file_size = latest_log.stat().st_size / 1024  # Size in KB
                    print(f"      {latest_log.name} ({file_size:.2f} KB)")
            else:
                print("   WARNING: No log files found")
        print()

        # Instructions
        print("=" * 80)
        print("NEXT STEPS")
        print("=" * 80 + "\n")
        print("To view Allure HTML report, run:")
        print(f"   allure serve {self.allure_results_dir}\n")
        print("Or generate static HTML report:")
        print(f"   allure generate {self.allure_results_dir} -o {self.reports_dir / 'allure_html'} --clean\n")
        print("=" * 80 + "\n")

    def build_behave_command(self, tags=None, features=None, dry_run=False, parallel=False):
        """
        Build behave command with arguments

        Args:
            tags: List of tags to filter tests
            features: Specific feature files to run
            dry_run: Run in dry-run mode
            parallel: Run tests in parallel

        Returns:
            Command list for subprocess
        """
        cmd = ["behave"]

        # Add Allure formatter for proper Allure reporting
        cmd.extend(["--format", "allure_behave.formatter:AllureFormatter"])
        cmd.extend(["--outfile", str(self.allure_results_dir)])

        # Add plain text formatter (console output)
        cmd.extend(["--format", "plain"])

        # Add tags
        if tags:
            for tag in tags:
                cmd.extend(["-t", tag])

        # Add features
        if features:
            for feature in features:
                cmd.append(feature)

        # Add options
        if dry_run:
            cmd.append("--dry-run")

        if parallel:
            cmd.append("--parallel")

        return cmd

    def run_tests(self, tags=None, features=None, dry_run=False, parallel=False, verbose=False):
        """
        Run behave tests

        Args:
            tags: List of tags to filter tests
            features: Specific feature files to run
            dry_run: Run in dry-run mode
            parallel: Run tests in parallel
            verbose: Print verbose output

        Returns:
            Return code from behave execution
        """
        self.start_time = datetime.now()

        # Setup directories
        self.setup_directories()

        # Print header
        self.print_header()

        # Build command
        cmd = self.build_behave_command(tags, features, dry_run, parallel)

        # Print command
        if verbose:
            print("Running command:")
            print(f"   {' '.join(cmd)}\n")

        # Print test configuration
        print("TEST CONFIGURATION:")
        print("=" * 80)
        print(f"  Tags:        {', '.join(tags) if tags else 'All tests'}")
        print(f"  Features:    {', '.join(features) if features else 'All features'}")
        print(f"  Dry Run:     {dry_run}")
        print(f"  Parallel:    {parallel}")
        print(f"  Reports Dir: {self.reports_dir}")
        print(f"  Logs Dir:    {self.logs_dir}")
        print("=" * 80 + "\n")

        # Run tests
        print("Running tests...")
        print("=" * 80 + "\n")

        try:
            result = subprocess.run(cmd, cwd=str(self.project_root))
        except KeyboardInterrupt:
            print("\n\nWARNING: Test execution interrupted by user")
            result = subprocess.CompletedProcess(args=cmd, returncode=1)
        except Exception as e:
            print(f"\n\nERROR: Error running tests: {str(e)}")
            result = subprocess.CompletedProcess(args=cmd, returncode=1)

        # Print footer
        self.print_footer(result.returncode)

        # Print report locations
        self.print_report_locations()

        return result.returncode

    def run_smoke_tests(self):
        """Run only smoke tests"""
        print("\nRunning SMOKE tests...\n")
        return self.run_tests(tags=["@smoke"])

    def run_critical_tests(self):
        """Run only critical tests"""
        print("\nRunning CRITICAL tests...\n")
        return self.run_tests(tags=["@critical"])

    def run_regression_tests(self):
        """Run only regression tests"""
        print("\nRunning REGRESSION tests...\n")
        return self.run_tests(tags=["@regression"])

    def run_specific_feature(self, feature_file):
        """
        Run specific feature file

        Args:
            feature_file: Path to feature file
        """
        print(f"\nRunning feature: {feature_file}\n")
        return self.run_tests(features=[feature_file])

    def run_all_tests(self):
        """Run all tests"""
        print("\nRunning ALL tests...\n")
        return self.run_tests()

    def dry_run(self, tags=None):
        """
        Run tests in dry-run mode

        Args:
            tags: List of tags to filter tests
        """
        print("\nRunning DRY RUN...\n")
        return self.run_tests(tags=tags, dry_run=True)

    def run_parallel(self, tags=None):
        """
        Run tests in parallel

        Args:
            tags: List of tags to filter tests
        """
        print("\nRunning tests in PARALLEL...\n")
        return self.run_tests(tags=tags, parallel=True)


def main():
    """Main function"""
    parser = argparse.ArgumentParser(
        description="Test Runner for DEMOQA Automation Framework",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python run_tests.py                          # Run all tests
  python run_tests.py --smoke                  # Run smoke tests
  python run_tests.py --critical               # Run critical tests
  python run_tests.py --regression             # Run regression tests
  python run_tests.py --tags @smoke @critical  # Run multiple tags
  python run_tests.py --feature features/practice_forms_field_validation.feature
  python run_tests.py --dry-run                # Dry run
  python run_tests.py --parallel               # Run in parallel
  python run_tests.py --verbose                # Verbose output
        """,
    )

    # Add arguments
    parser.add_argument("--smoke", action="store_true", help="Run only smoke tests (@smoke)")
    parser.add_argument("--critical", action="store_true", help="Run only critical tests (@critical)")
    parser.add_argument("--regression", action="store_true", help="Run only regression tests (@regression)")
    parser.add_argument("--tags", nargs="+", help="Run tests with specific tags (e.g., @smoke @positive)")
    parser.add_argument("--feature", help="Run specific feature file")
    parser.add_argument("--dry-run", action="store_true", help="Run in dry-run mode (no actual execution)")
    parser.add_argument("--parallel", action="store_true", help="Run tests in parallel")
    parser.add_argument("--verbose", "-v", action="store_true", help="Print verbose output")

    # Parse arguments
    args = parser.parse_args()

    # Create test runner
    runner = TestRunner()

    # Execute based on arguments
    if args.smoke:
        return_code = runner.run_smoke_tests()
    elif args.critical:
        return_code = runner.run_critical_tests()
    elif args.regression:
        return_code = runner.run_regression_tests()
    elif args.tags:
        return_code = runner.run_tests(tags=args.tags, verbose=args.verbose)
    elif args.feature:
        return_code = runner.run_specific_feature(args.feature)
    elif args.dry_run:
        return_code = runner.dry_run()
    elif args.parallel:
        return_code = runner.run_parallel()
    else:
        return_code = runner.run_all_tests()

    return return_code


if __name__ == "__main__":
    sys.exit(main())
